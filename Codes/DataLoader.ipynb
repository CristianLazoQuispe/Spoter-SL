{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e275229a-dd65-461e-879c-26584ceb231e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "300a910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../Src/')\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import logging\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "#from utils import __balance_val_split, __split_of_train_sequence, __log_class_statistics\n",
    "from czech_slr_dataset import CzechSLRDataset,GaussianNoise\n",
    "#from spoter_model import SPOTER\n",
    "#from spoter.utils import train_epoch, evaluate\n",
    "#from spoter.gaussian_noise import GaussianNoise\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "927c8249",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    def __init__(self):\n",
    "        self.seed = 379\n",
    "        self.testing_set_path = '../../DATASETS/AEC--openpose-val.csv'\n",
    "        self.training_set_path = '../../DATASETS/AEC--openpose-train.csv' \n",
    "        self.experiment_name = 'cris_openpose_AEC'\n",
    "        \n",
    "        self.gaussian_mean  = 0\n",
    "        self.gaussian_std  =0.001\n",
    "                        \n",
    "args = args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e40cc37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0188294570>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "g = torch.Generator()\n",
    "g.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aea65f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to CUDA only if applicable\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d3000b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set labels\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n",
      "BODY_IDENTIFIERS ['k0', 'k16', 'k15', 'k5', 'k2', 'k6', 'k3', 'k7', 'k4', 'k75', 'k46', 'k73', 'k42', 'k81', 'k44', 'k61', 'k28', 'k64', 'k32', 'k30', 'k77', 'k47', 'k79', 'k51', 'k83', 'k49', 'k70', 'k38', 'k67', 'k34', 'k36', 'k96', 'k97', 'k98', 'k99', 'k100', 'k101', 'k102', 'k103', 'k104', 'k105', 'k106', 'k107', 'k108', 'k109', 'k110', 'k111', 'k112', 'k113', 'k114', 'k115', 'k117', 'k118', 'k119', 'k120', 'k121', 'k122']\n",
      "HAND_IDENTIFIERS ['k123', 'k124', 'k125', 'k126', 'k127', 'k128', 'k129', 'k130', 'k131', 'k132', 'k133', 'k134', 'k135', 'k136']\n"
     ]
    }
   ],
   "source": [
    "transform    = transforms.Compose([GaussianNoise(args.gaussian_mean, args.gaussian_std)])\n",
    "\n",
    "train_set    = CzechSLRDataset(args.training_set_path, transform=transform, augmentations=False)\n",
    "train_loader = DataLoader(train_set, shuffle=True, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6b80c3b6-039c-4765-9a1b-e9ca8092b863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([17.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[130][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d028294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set labels\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n",
      "BODY_IDENTIFIERS ['k0', 'k16', 'k15', 'k5', 'k2', 'k6', 'k3', 'k7', 'k4', 'k75', 'k46', 'k73', 'k42', 'k81', 'k44', 'k61', 'k28', 'k64', 'k32', 'k30', 'k77', 'k47', 'k79', 'k51', 'k83', 'k49', 'k70', 'k38', 'k67', 'k34', 'k36', 'k96', 'k97', 'k98', 'k99', 'k100', 'k101', 'k102', 'k103', 'k104', 'k105', 'k106', 'k107', 'k108', 'k109', 'k110', 'k111', 'k112', 'k113', 'k114', 'k115', 'k117', 'k118', 'k119', 'k120', 'k121', 'k122']\n",
      "HAND_IDENTIFIERS ['k123', 'k124', 'k125', 'k126', 'k127', 'k128', 'k129', 'k130', 'k131', 'k132', 'k133', 'k134', 'k135', 'k136']\n"
     ]
    }
   ],
   "source": [
    "eval_set    = CzechSLRDataset(args.testing_set_path)\n",
    "eval_loader = DataLoader(eval_set, shuffle=True, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a502b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 71, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[3][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8abd728b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_loader.dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cf161c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1486ee9c-2e41-474a-92e0-e3388d20fc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import glob\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import time\n",
    "import gc\n",
    "from collections import Counter\n",
    "\n",
    "    \n",
    "paths = glob.glob('../../DATASETS/*.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ec8adcb-c921-4945-8a06-e15dcd9db652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_h5(path):\n",
    "    hf = h5py.File(path, 'r')\n",
    "    return hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "558e4b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading : ../../DATASETS/AEC--mediapipe-Train.hdf5\n",
      "(14, 2, 543) aprender\n",
      "reading : ../../DATASETS/AEC--mediapipe-Val.hdf5\n",
      "(17, 2, 543) hacer\n",
      "reading : ../../DATASETS/AEC--openpose-Train.hdf5\n",
      "(11, 2, 137) aprender\n",
      "reading : ../../DATASETS/AEC--openpose-Val.hdf5\n",
      "(12, 2, 137) hacer\n",
      "reading : ../../DATASETS/AEC--openpose.hdf5\n",
      "(12, 2, 137) antes\n",
      "reading : ../../DATASETS/AEC--wholepose-Train.hdf5\n",
      "(14, 2, 133) aprender\n",
      "reading : ../../DATASETS/AEC--wholepose-Val.hdf5\n",
      "(17, 2, 133) hacer\n",
      "reading : ../../DATASETS/PUCP_PSL_DGI156--mediapipe-Train.hdf5\n",
      "(43, 2, 543) mujer\n",
      "reading : ../../DATASETS/PUCP_PSL_DGI156--mediapipe-Val.hdf5\n",
      "(22, 2, 543) mamá\n",
      "reading : ../../DATASETS/PUCP_PSL_DGI156--openpose-Train.hdf5\n",
      "(9, 2, 137) mujer\n",
      "reading : ../../DATASETS/PUCP_PSL_DGI156--openpose-Val.hdf5\n",
      "(12, 2, 137) mamá\n",
      "reading : ../../DATASETS/PUCP_PSL_DGI156--wholepose-Train.hdf5\n",
      "(43, 2, 133) mujer\n",
      "reading : ../../DATASETS/PUCP_PSL_DGI156--wholepose-Val.hdf5\n",
      "(22, 2, 133) mamá\n",
      "reading : ../../DATASETS/WLASL--mediapipe-Train.hdf5\n",
      "(84, 2, 543) wife\n",
      "reading : ../../DATASETS/WLASL--mediapipe-Val.hdf5\n",
      "(61, 2, 543) hearing\n",
      "reading : ../../DATASETS/WLASL--openpose-Train.hdf5\n",
      "(86, 2, 137) book\n",
      "reading : ../../DATASETS/WLASL--openpose-Val.hdf5\n",
      "(77, 2, 137) tell\n",
      "reading : ../../DATASETS/WLASL--wholepose-Train.hdf5\n",
      "(84, 2, 133) wife\n",
      "reading : ../../DATASETS/WLASL--wholepose-Val.hdf5\n",
      "(61, 2, 133) hearing\n"
     ]
    }
   ],
   "source": [
    "for path in paths:\n",
    "    print('reading :',path)\n",
    "    data = get_data_from_h5(path)\n",
    "    #torch.Size([5, 71, 2])\n",
    "    data_video = np.array(data['0']['data'])\n",
    "    data_label = np.array(data['0']['label']).item().decode('utf-8')\n",
    "    #data = data_video.reshape((12,137,2))\n",
    "    print(data_video.shape,data_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c4aa68-db8b-4f40-ace9-9066f036f179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48f977d0-bdaf-4d7d-b380-c19ff46271a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_dataset_from_hdf5(path,keypoints_model,threshold_frecuency_labels=10,list_labels_banned=[]):\n",
    "\n",
    "    index_array_column = None #'mp_indexInArray', 'wp_indexInArray','op_indexInArray'\n",
    "\n",
    "    print('Use keypoint model : ',keypoints_model) \n",
    "    if keypoints_model == 'openpose':\n",
    "        index_array_column  = 'op_indexInArray'\n",
    "    if keypoints_model == 'mediapipe':\n",
    "        index_array_column  = 'mp_indexInArray'\n",
    "    if keypoints_model == 'wholepose':\n",
    "        index_array_column  = 'wp_indexInArray'\n",
    "    print('use column for index keypoint :',index_array_column)\n",
    "\n",
    "    assert not index_array_column is None\n",
    "\n",
    "    df_keypoints = pd.read_csv('../../DATASETS/Mapeo landmarks librerias - Hoja 1.csv', skiprows=1)\n",
    "    df_keypoints = df_keypoints[(df_keypoints['Selected73']=='x' )& (df_keypoints['Key']!='wrist')]\n",
    "    idx_keypoints = sorted(df_keypoints[index_array_column].astype(int).values)\n",
    "    name_keypoints = df_keypoints['Key'].values\n",
    "    section_keypoints = (df_keypoints['Section']+'_'+df_keypoints['Key']).values\n",
    "    print('section_keypoints : ',len(section_keypoints),' -- uniques: ',len(set(section_keypoints)))\n",
    "    print('name_keypoints    : ',len(name_keypoints),' -- uniques: ',len(set(name_keypoints)))\n",
    "    print('idx_keypoints     : ',len(idx_keypoints),' -- uniques: ',len(set(idx_keypoints)))\n",
    "    print('')\n",
    "    print('section_keypoints used:')\n",
    "    print(section_keypoints)\n",
    "\n",
    "\n",
    "    print('Reading dataset .. ')\n",
    "    data = get_data_from_h5(path)\n",
    "    #torch.Size([5, 71, 2])\n",
    "    print('Total size dataset : ',len(data.keys()))\n",
    "    video_dataset  = []\n",
    "    labels_dataset = []\n",
    "\n",
    "    time.sleep(2)\n",
    "    for index in tqdm.tqdm(list(data.keys())):\n",
    "\n",
    "        data_video = np.array(data[index]['data'])\n",
    "        data_label = np.array(data[index]['label']).item().decode('utf-8')\n",
    "\n",
    "        n_frames,n_axis,n_keypoints = data_video.shape\n",
    "\n",
    "        data_video = data_video.reshape((n_frames,n_keypoints,n_axis))\n",
    "        if index=='0':\n",
    "            print('original size video : ',data_video.shape,'-- label : ',data_label)\n",
    "            print('filtering by keypoints idx .. ')\n",
    "        data_video = data_video[:,idx_keypoints,:]\n",
    "        if index=='0':\n",
    "            print('filtered size video : ',data_video.shape,'-- label : ',data_label)\n",
    "\n",
    "        video_dataset.append(data_video)\n",
    "        labels_dataset.append(data_label)\n",
    "    del data\n",
    "    gc.collect()\n",
    "    \n",
    "    print('label encoding ...')\n",
    "    dict_labels_dataset = {}\n",
    "    inv_dict_labels_dataset = {}\n",
    "\n",
    "    for index,label in enumerate(set(labels_dataset)):\n",
    "        dict_labels_dataset[label] = index\n",
    "        inv_dict_labels_dataset[index] = label\n",
    "        \n",
    "    encoded_dataset = [dict_labels_dataset[label] for label in labels_dataset]\n",
    "    \n",
    "    print('label encoding completed!')\n",
    "    \n",
    "    print('frecuency labels filtering ...')\n",
    "    hist_labels = dict(Counter(labels_dataset))\n",
    "    labels_high_frecuency = []\n",
    "    for name in hist_labels.keys():\n",
    "        if hist_labels[name] >= threshold_frecuency_labels and not name in list_labels_banned:\n",
    "            labels_high_frecuency.append(name)\n",
    "    labels_high_frecuency = sorted(labels_high_frecuency)\n",
    "    len(labels_high_frecuency)\n",
    "    len(set(labels_dataset))\n",
    "    filtros = [label in labels_high_frecuency for label in labels_dataset]\n",
    "    \n",
    "    print('before filter size video_dataset   :',len(video_dataset))\n",
    "    print('before filter size labels_dataset  :',len(labels_dataset))\n",
    "    print('before filter size encoded_dataset :',len(encoded_dataset))\n",
    "    video_dataset   = np.array(video_dataset)[filtros]\n",
    "    labels_dataset  = np.array(labels_dataset)[filtros]\n",
    "    encoded_dataset = np.array(encoded_dataset)[filtros]\n",
    "    print('after  filter size video_dataset   :',len(video_dataset))\n",
    "    print('after  filter size labels_dataset  :',len(labels_dataset))\n",
    "    print('after  filter size encoded_dataset :',len(encoded_dataset))\n",
    "    print('frecuency labels completed!')\n",
    "\n",
    "    print('Reading dataset completed!')\n",
    "    \n",
    "    return video_dataset,labels_dataset,encoded_dataset,dict_labels_dataset,inv_dict_labels_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcfbdf2f-d464-44d6-bed5-7b1d99aaaa88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use keypoint model :  openpose\n",
      "use column for index keypoint : op_indexInArray\n",
      "section_keypoints :  71  -- uniques:  71\n",
      "name_keypoints    :  71  -- uniques:  51\n",
      "idx_keypoints     :  71  -- uniques:  71\n",
      "\n",
      "section_keypoints used:\n",
      "['pose_nose' 'pose_left_eye' 'pose_right_eye' 'pose_left_shoulder'\n",
      " 'pose_right_shoulder' 'pose_left_elbow' 'pose_right_elbow'\n",
      " 'pose_left_wrist' 'pose_right_wrist' 'face_right_mouth_up'\n",
      " 'face_right_eyebrow_inner' 'face_right_mouth_corner'\n",
      " 'face_right_eyebrow_outer' 'face_right_mouth_down'\n",
      " 'face_right_eyebrow_middle' 'face_right_eye_outer' 'face_right_jaw_up'\n",
      " 'face_right_eye_inner' 'face_right_jaw_down' 'face_right_jaw_middle'\n",
      " 'face_left_mouth_up' 'face_left_eyebrow_inner' 'face_left_mouth_corner'\n",
      " 'face_left_eyebrow_outer' 'face_left_mouth_down'\n",
      " 'face_left_eyebrow_middle' 'face_left_eye_outer' 'face_left_jaw_up'\n",
      " 'face_left_eye_inner' 'face_left_jaw_down' 'face_left_jaw_middle'\n",
      " 'leftHand_thumb_cmc' 'leftHand_thumb_mcp' 'leftHand_thumb_ip'\n",
      " 'leftHand_thumb_tip' 'leftHand_index_finger_mcp'\n",
      " 'leftHand_index_finger_pip' 'leftHand_index_finger_dip'\n",
      " 'leftHand_index_finger_tip' 'leftHand_middle_finger_mcp'\n",
      " 'leftHand_middle_finger_pip' 'leftHand_middle_finger_dip'\n",
      " 'leftHand_middle_finger_tip' 'leftHand_ring_finger_mcp'\n",
      " 'leftHand_ring_finger_pip' 'leftHand_ring_finger_dip'\n",
      " 'leftHand_ring_finger_tip' 'leftHand_pinky_mcp' 'leftHand_pinky_pip'\n",
      " 'leftHand_pinky_dip' 'leftHand_pinky_tip' 'rightHand_thumb_cmc'\n",
      " 'rightHand_thumb_mcp' 'rightHand_thumb_ip' 'rightHand_thumb_tip'\n",
      " 'rightHand_index_finger_mcp' 'rightHand_index_finger_pip'\n",
      " 'rightHand_index_finger_dip' 'rightHand_index_finger_tip'\n",
      " 'rightHand_middle_finger_mcp' 'rightHand_middle_finger_pip'\n",
      " 'rightHand_middle_finger_dip' 'rightHand_middle_finger_tip'\n",
      " 'rightHand_ring_finger_mcp' 'rightHand_ring_finger_pip'\n",
      " 'rightHand_ring_finger_dip' 'rightHand_ring_finger_tip'\n",
      " 'rightHand_pinky_mcp' 'rightHand_pinky_pip' 'rightHand_pinky_dip'\n",
      " 'rightHand_pinky_tip']\n",
      "Reading dataset .. \n",
      "Total size dataset :  625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|███████████████████████▋               | 380/625 [00:00<00:00, 1637.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original size video :  (11, 137, 2) -- label :  aprender\n",
      "filtering by keypoints idx .. \n",
      "filtered size video :  (11, 71, 2) -- label :  aprender\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 625/625 [00:00<00:00, 2035.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label encoding ...\n",
      "label encoding completed!\n",
      "frecuency labels filtering ...\n",
      "before filter size video_dataset   : 625\n",
      "before filter size labels_dataset  : 625\n",
      "before filter size encoded_dataset : 625\n",
      "after  filter size video_dataset   : 625\n",
      "after  filter size labels_dataset  : 625\n",
      "after  filter size encoded_dataset : 625\n",
      "frecuency labels completed!\n",
      "Reading dataset completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_12419/1576412037.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  video_dataset   = np.array(video_dataset)[filtros]\n"
     ]
    }
   ],
   "source": [
    "path = '../../DATASETS/AEC--openpose-Train.hdf5'\n",
    "keypoints_model = 'openpose' # mediapipe, wholepose\n",
    "\n",
    "video_dataset,labels_dataset,encoded_dataset,dict_labels_dataset,inv_dict_labels_dataset = get_dataset_from_hdf5(path,keypoints_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af3bf665-547c-43c1-b9c4-fd7c1c3c6254",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSP_Dataset(Dataset):\n",
    "    \"\"\"Advanced object representation of the HPOES dataset for loading hand joints landmarks utilizing the Torch's\n",
    "    built-in Dataset properties\"\"\"\n",
    "\n",
    "    data: [np.ndarray]  # type: ignore\n",
    "    labels: [np.ndarray]  # type: ignore\n",
    "\n",
    "    def __init__(self, dataset_filename: str,keypoints_model:str, num_labels=5, transform=None, augmentations=False,\n",
    "                 augmentations_prob=0.5, normalize=False):\n",
    "        \"\"\"\n",
    "        Initiates the HPOESDataset with the pre-loaded data from the h5 file.\n",
    "\n",
    "        :param dataset_filename: Path to the h5 file\n",
    "        :param transform: Any data transformation to be applied (default: None)\n",
    "        \"\"\"\n",
    "\n",
    "        self.list_labels_banned = [\"ya\", \"qué?\", \"qué\", \"bien\", \"dos\", \"ahí\", \"luego\", \"yo\", \"él\", \"tú\",\"???\",\"NNN\"]\n",
    "\n",
    "        video_dataset,labels_dataset,encoded_dataset,dict_labels_dataset,inv_dict_labels_dataset = get_dataset_from_hdf5(dataset_filename,keypoints_model,\n",
    "                                                                                         threshold_frecuency_labels =10,\n",
    "                                                                                         list_labels_banned =self.list_labels_banned)\n",
    "\n",
    "        self.data = video_dataset\n",
    "        self.labels = encoded_dataset\n",
    "        #self.targets = list(encoded_dataset)\n",
    "        self.text_labels = list(labels_dataset)\n",
    "        self.num_labels = num_labels\n",
    "        self.transform = transform\n",
    "\n",
    "        self.augmentations = augmentations\n",
    "        self.augmentations_prob = augmentations_prob\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Allocates, potentially transforms and returns the item at the desired index.\n",
    "\n",
    "        :param idx: Index of the item\n",
    "        :return: Tuple containing both the depth map and the label\n",
    "        \"\"\"\n",
    "        depth_map = torch.from_numpy(np.copy(self.data[idx]))\n",
    "        label = torch.Tensor([self.labels[idx] - 1])\n",
    "        depth_map = depth_map - 0.5\n",
    "        if self.transform:\n",
    "            depth_map = self.transform(depth_map)\n",
    "        return depth_map, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d12149a-d3e8-4fac-9a5c-d0e012d43856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use keypoint model :  openpose\n",
      "use column for index keypoint : op_indexInArray\n",
      "section_keypoints :  71  -- uniques:  71\n",
      "name_keypoints    :  71  -- uniques:  51\n",
      "idx_keypoints     :  71  -- uniques:  71\n",
      "\n",
      "section_keypoints used:\n",
      "['pose_nose' 'pose_left_eye' 'pose_right_eye' 'pose_left_shoulder'\n",
      " 'pose_right_shoulder' 'pose_left_elbow' 'pose_right_elbow'\n",
      " 'pose_left_wrist' 'pose_right_wrist' 'face_right_mouth_up'\n",
      " 'face_right_eyebrow_inner' 'face_right_mouth_corner'\n",
      " 'face_right_eyebrow_outer' 'face_right_mouth_down'\n",
      " 'face_right_eyebrow_middle' 'face_right_eye_outer' 'face_right_jaw_up'\n",
      " 'face_right_eye_inner' 'face_right_jaw_down' 'face_right_jaw_middle'\n",
      " 'face_left_mouth_up' 'face_left_eyebrow_inner' 'face_left_mouth_corner'\n",
      " 'face_left_eyebrow_outer' 'face_left_mouth_down'\n",
      " 'face_left_eyebrow_middle' 'face_left_eye_outer' 'face_left_jaw_up'\n",
      " 'face_left_eye_inner' 'face_left_jaw_down' 'face_left_jaw_middle'\n",
      " 'leftHand_thumb_cmc' 'leftHand_thumb_mcp' 'leftHand_thumb_ip'\n",
      " 'leftHand_thumb_tip' 'leftHand_index_finger_mcp'\n",
      " 'leftHand_index_finger_pip' 'leftHand_index_finger_dip'\n",
      " 'leftHand_index_finger_tip' 'leftHand_middle_finger_mcp'\n",
      " 'leftHand_middle_finger_pip' 'leftHand_middle_finger_dip'\n",
      " 'leftHand_middle_finger_tip' 'leftHand_ring_finger_mcp'\n",
      " 'leftHand_ring_finger_pip' 'leftHand_ring_finger_dip'\n",
      " 'leftHand_ring_finger_tip' 'leftHand_pinky_mcp' 'leftHand_pinky_pip'\n",
      " 'leftHand_pinky_dip' 'leftHand_pinky_tip' 'rightHand_thumb_cmc'\n",
      " 'rightHand_thumb_mcp' 'rightHand_thumb_ip' 'rightHand_thumb_tip'\n",
      " 'rightHand_index_finger_mcp' 'rightHand_index_finger_pip'\n",
      " 'rightHand_index_finger_dip' 'rightHand_index_finger_tip'\n",
      " 'rightHand_middle_finger_mcp' 'rightHand_middle_finger_pip'\n",
      " 'rightHand_middle_finger_dip' 'rightHand_middle_finger_tip'\n",
      " 'rightHand_ring_finger_mcp' 'rightHand_ring_finger_pip'\n",
      " 'rightHand_ring_finger_dip' 'rightHand_ring_finger_tip'\n",
      " 'rightHand_pinky_mcp' 'rightHand_pinky_pip' 'rightHand_pinky_dip'\n",
      " 'rightHand_pinky_tip']\n",
      "Reading dataset .. \n",
      "Total size dataset :  625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|████████████████████████▌              | 394/625 [00:00<00:00, 1762.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original size video :  (11, 137, 2) -- label :  aprender\n",
      "filtering by keypoints idx .. \n",
      "filtered size video :  (11, 71, 2) -- label :  aprender\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 625/625 [00:00<00:00, 2091.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label encoding ...\n",
      "label encoding completed!\n",
      "frecuency labels filtering ...\n",
      "before filter size video_dataset   : 625\n",
      "before filter size labels_dataset  : 625\n",
      "before filter size encoded_dataset : 625\n",
      "after  filter size video_dataset   : 625\n",
      "after  filter size labels_dataset  : 625\n",
      "after  filter size encoded_dataset : 625\n",
      "frecuency labels completed!\n",
      "Reading dataset completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12419/1576412037.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  video_dataset   = np.array(video_dataset)[filtros]\n"
     ]
    }
   ],
   "source": [
    "path = '../../DATASETS/AEC--openpose-Train.hdf5'\n",
    "keypoints_model = 'openpose' # mediapipe, wholepose\n",
    "\n",
    "transform    = transforms.Compose([GaussianNoise(args.gaussian_mean, args.gaussian_std)])\n",
    "\n",
    "train_set    = LSP_Dataset(path,keypoints_model, transform=transform, augmentations=False)\n",
    "train_loader = DataLoader(train_set, shuffle=True, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69e24058-ac43-4152-9e69-7d88df7a8b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 71, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "583428e4-46c5-4a7e-9d19-c0f316a165f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[10][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4da790b9-85cd-4311-8033-34472accc12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Lsp_dataset import LSP_Dataset as test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "58d0dcbb-c647-415a-b0e0-7213d560a998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use keypoint model :  openpose\n",
      "use column for index keypoint : op_indexInArray\n",
      "section_keypoints :  71  -- uniques:  71\n",
      "name_keypoints    :  71  -- uniques:  51\n",
      "idx_keypoints     :  71  -- uniques:  71\n",
      "\n",
      "section_keypoints used:\n",
      "['pose_nose' 'pose_left_eye' 'pose_right_eye' 'pose_left_shoulder'\n",
      " 'pose_right_shoulder' 'pose_left_elbow' 'pose_right_elbow'\n",
      " 'pose_left_wrist' 'pose_right_wrist' 'face_right_mouth_up'\n",
      " 'face_right_eyebrow_inner' 'face_right_mouth_corner'\n",
      " 'face_right_eyebrow_outer' 'face_right_mouth_down'\n",
      " 'face_right_eyebrow_middle' 'face_right_eye_outer' 'face_right_jaw_up'\n",
      " 'face_right_eye_inner' 'face_right_jaw_down' 'face_right_jaw_middle'\n",
      " 'face_left_mouth_up' 'face_left_eyebrow_inner' 'face_left_mouth_corner'\n",
      " 'face_left_eyebrow_outer' 'face_left_mouth_down'\n",
      " 'face_left_eyebrow_middle' 'face_left_eye_outer' 'face_left_jaw_up'\n",
      " 'face_left_eye_inner' 'face_left_jaw_down' 'face_left_jaw_middle'\n",
      " 'leftHand_thumb_cmc' 'leftHand_thumb_mcp' 'leftHand_thumb_ip'\n",
      " 'leftHand_thumb_tip' 'leftHand_index_finger_mcp'\n",
      " 'leftHand_index_finger_pip' 'leftHand_index_finger_dip'\n",
      " 'leftHand_index_finger_tip' 'leftHand_middle_finger_mcp'\n",
      " 'leftHand_middle_finger_pip' 'leftHand_middle_finger_dip'\n",
      " 'leftHand_middle_finger_tip' 'leftHand_ring_finger_mcp'\n",
      " 'leftHand_ring_finger_pip' 'leftHand_ring_finger_dip'\n",
      " 'leftHand_ring_finger_tip' 'leftHand_pinky_mcp' 'leftHand_pinky_pip'\n",
      " 'leftHand_pinky_dip' 'leftHand_pinky_tip' 'rightHand_thumb_cmc'\n",
      " 'rightHand_thumb_mcp' 'rightHand_thumb_ip' 'rightHand_thumb_tip'\n",
      " 'rightHand_index_finger_mcp' 'rightHand_index_finger_pip'\n",
      " 'rightHand_index_finger_dip' 'rightHand_index_finger_tip'\n",
      " 'rightHand_middle_finger_mcp' 'rightHand_middle_finger_pip'\n",
      " 'rightHand_middle_finger_dip' 'rightHand_middle_finger_tip'\n",
      " 'rightHand_ring_finger_mcp' 'rightHand_ring_finger_pip'\n",
      " 'rightHand_ring_finger_dip' 'rightHand_ring_finger_tip'\n",
      " 'rightHand_pinky_mcp' 'rightHand_pinky_pip' 'rightHand_pinky_dip'\n",
      " 'rightHand_pinky_tip']\n",
      "Reading dataset .. \n",
      "Total size dataset :  625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|█████████████████████████████▉         | 479/625 [00:00<00:00, 2397.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original size video :  (11, 137, 2) -- label :  aprender\n",
      "filtering by keypoints idx .. \n",
      "filtered size video :  (11, 71, 2) -- label :  aprender\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 625/625 [00:00<00:00, 2338.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label encoding ...\n",
      "label encoding completed!\n",
      "frecuency labels filtering ...\n",
      "hist counter\n",
      "{'aprender': 18, 'hacer': 14, 'tres': 17, 'proteína': 30, 'sí': 36, 'cómo': 19, 'comer': 58, 'cuánto': 36, 'porcentaje': 44, 'dentro': 14, 'decir': 17, 'cero': 24, 'ahora': 15, 'hermano': 26, 'no': 22, 'pensar': 25, 'niño': 12, 'manejar': 12, 'ver': 18, 'G-R': 27, 'uno': 18, 'emoción': 23, 'ese': 19, 'cien': 24, 'fuerte': 17, 'solucionar': 14, 'conflicto': 13, 'importante': 13}\n",
      "total unique labels :  28\n",
      "before filter size video_dataset   : 625\n",
      "before filter size labels_dataset  : 625\n",
      "before filter size encoded_dataset : 625\n",
      "after  filter size video_dataset   : 625\n",
      "after  filter size labels_dataset  : 625\n",
      "after  filter size encoded_dataset : 625\n",
      "frecuency labels completed!\n",
      "Reading dataset completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/cristian/Extreme/Investigacion/SL/Spoter-SL/Codes/../Src/Lsp_dataset.py:105: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  print('Reading dataset completed!')\n"
     ]
    }
   ],
   "source": [
    "path = '../../DATASETS/AEC--openpose-Train.hdf5'\n",
    "keypoints_model = 'openpose' # mediapipe, wholepose\n",
    "\n",
    "transform    = transforms.Compose([GaussianNoise(args.gaussian_mean, args.gaussian_std)])\n",
    "\n",
    "train_set    = test_loader(path,keypoints_model, transform=transform, augmentations=False,landmarks_ref= '../../DATASETS/Mapeo landmarks librerias - Hoja 1.csv')\n",
    "train_loader = DataLoader(train_set, shuffle=True, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0ebaec85-ab72-4f07-9197-aba2f8e4cda4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14.])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[130][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c0fc2d-a5fb-4bb4-a510-263dad1589ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a1c92e809b6616072452fe5a6e71d24a512e64da11fd113da9762fcc06817e1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
