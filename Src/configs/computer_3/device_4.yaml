program: train.py
method: grid
parameters:
  experiment_name:
    values: [Testing new 5 folds seed 42 -c3nvlink]
    
  training_set_path:
    values: ["../SL_ConnectingPoints/split/DGI305-AEC--38--incremental--mediapipe_seed_42_klod_1-Train.hdf5",
    "../SL_ConnectingPoints/split/DGI305-AEC--38--incremental--mediapipe_seed_42_klod_2-Train.hdf5",
    "../SL_ConnectingPoints/split/DGI305-AEC--38--incremental--mediapipe_seed_42_klod_3-Train.hdf5",
    "../SL_ConnectingPoints/split/DGI305-AEC--38--incremental--mediapipe_seed_42_klod_4-Train.hdf5",
    "../SL_ConnectingPoints/split/DGI305-AEC--38--incremental--mediapipe_seed_42_klod_5-Train.hdf5",]
  validation_set_path:
    values: [""]
  sweep:
    values: [1]
  lr:
    values: [0.0001] #0.001, 0.00001,
  num_heads:
    values: [9] # [54,27,9,3,1] 27,18,9,6,3,2,1] este debe ser un divisor de hidden_dim  : nhead
  num_layers_1:
    values: [3] # [64,12,8,6,4,2,1]
  num_layers_2:
    values: [27] # [64,12,8,6,4,2,1] # self.transformer.decoder.num_layers :n_clones
  dim_feedforward:
    values: [8] #
  device:
    values: [0]
  augmentation:
    values: [1]
  factor_aug:
    values: [1]
  batch_name:
    values: [mean_2]
  batch_size:
    values: [32]
  loss_weighted_factor:
    values: [2]
  label_smoothing:
    values: [0.1]
  early_stopping_patience:
    values: [50]
  epochs:
    values: [800]
  scheduler:
    values: ["plateu"]
  optimizer:
    values: ['adam']
  weight_decay:
    values: [0.0001]
  dropout:
    values: [0.3]
  gaussian_std:
    values: [0.1]

#python train.py --device=0 --dim_feedforward=128 --lr=0.0001 --num_heads=54 --num_layers_1=32 --num_layers_2=256 --sweep=1 --augmentation=1
