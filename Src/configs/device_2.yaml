program: train.py
method: grid
parameters:
  experiment_name:
    values: [Aug:No_Loss:w.001_mean:16_pow2_Lr:plateu_Op:adam_Dout:0.3_Dim8-16]
  sweep:
    values: [1]
  lr:
    values: [0.001] #0.001, 0.00001,
  num_heads:
    values: [2,3,27] # [54,27,9,3,1] 27,18,9,6,3,2,1] este debe ser un divisor de hidden_dim  : nhead
  num_layers_1:
    values: [3,6,9,27] # [64,12,8,6,4,2,1]
  num_layers_2:
    values: [2,3,27] # [128,64,32,16,4,,64,12,8,6,4,2,1] # self.transformer.decoder.num_layers :n_clones
  dim_feedforward:
    values: [8,16] #
  device:
    values: [0]
  augmentation:
    values: [0]
  factor_aug:
    values: [1]
  batch_mean:
    values: [1]
  batch_size:
    values: [16]
  loss_weighted_factor:
    values: [2]
  label_smoothing:
    values: [0.1]
  early_stopping_patience:
    values: [500]
  epochs:
    values: [50000]
  scheduler:
    values: ["plateu"]
  optimizer:
    values: ['adam']
  weight_decay:
    values: [0.0001]
  dropout:
    values: [0.3]
#python train.py --device=0 --dim_feedforward=128 --lr=0.0001 --num_heads=54 --num_layers_1=32 --num_layers_2=256 --sweep=1 --augmentation=1
