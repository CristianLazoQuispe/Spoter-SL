program: train.py
method: grid
parameters:
  experiment_name:
    values: [Aug:No_Loss:w.01_pow3_Lr:pl_Op:adam_Dout:0.3]
  sweep:
    values: [1]
  lr:
    values: [0.01] #0.001, 0.00001,
  num_heads:
    values: [3,27] # [54,27,9,3,1] 27,18,9,6,3,2,1] este debe ser un divisor de hidden_dim  : nhead
  num_layers_1:
    values: [9,27] # [64,12,8,6,4,2,1]
  num_layers_2:
    values: [3,27] # [128,64,32,16,4,,64,12,8,6,4,2,1] # self.transformer.decoder.num_layers :n_clones
  dim_feedforward:
    values: [64,128] #
  device:
    values: [0]
  augmentation:
    values: [1]
  factor_aug:
    values: [1]
  batch_mean:
    values: [0]
  batch_size:
    values: [16]
  loss_weighted_factor:
    values: [3]
  label_smoothing:
    values: [0.1]
  early_stopping_patience:
    values: [500]
  epochs:
    values: [50000]
  scheduler:
    values: ["plateu"]
  optimizer:
    values: ['adam']
  weight_decay:
    values: [0.001]
  dropout:
    values: [0.3]
#python train.py --device=0 --dim_feedforward=128 --lr=0.0001 --num_heads=54 --num_layers_1=32 --num_layers_2=256 --sweep=1 --augmentation=1
